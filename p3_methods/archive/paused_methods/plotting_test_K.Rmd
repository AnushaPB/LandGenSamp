---
title: "test"
output: html_document
editor_options: 
  chunk_output_type: inline
---
```{r}

run_lfmm <- function(gen, gsd_df, loci_df, K = NULL){
  
  #for readibility, just negates the in function
  `%notin%` <- Negate(`%in%`)
  
  #get adaptive loci
  loci_trait1 <- loci_df$trait1 + 1 #add one to convert from python to R indexing
  loci_trait2 <- loci_df$trait2 + 1 #add one to convert from python to R indexing
  adaptive_loci <- c(loci_trait1, loci_trait2)
  neutral_loci <- c(1:nloci)[-adaptive_loci]
  
  #PCA to determine number of latent factors
  #if K is not specified it is calculated based on PCs
  if(is.null(K)){
    pc <- prcomp(gen)
    par(pty="s",mfrow=c(1,1))
    #if number of samples is greater than 100, only look at fits 100 PCs (shouldn't make a dif either way)
    if(nrow(gen)>100){
      eig <- pc$sdev[1:100]^2
    } else {
      eig <- pc$sdev^2
    }
    #estimate number of latent factors using quick.elbow (see general functions for description of how this function works)
    #this is a crude way to determine the number of latent factors that is based on an arbitrary "low" value 
    #(low defaults to 0.08, but this was too high imo so I changed it t0 0.05)
    
    K <- quick.elbow(eig, low = 0.1, max.pc = 0.8)
    var_explained = pc$sdev^2 / sum(pc$sdev^2)
    plot(var_explained, xlab = 'PC', ylab = "Variance explained")
    abline(v = K, col= "red", lty="dashed")
  }
}

```

```{r}

run_lfmm <- function(gen, gsd_df, loci_df, K = NULL){
  
  #for readibility, just negates the in function
  `%notin%` <- Negate(`%in%`)
  
  #get adaptive loci
  loci_trait1 <- loci_df$trait1 + 1 #add one to convert from python to R indexing
  loci_trait2 <- loci_df$trait2 + 1 #add one to convert from python to R indexing
  adaptive_loci <- c(loci_trait1, loci_trait2)
  neutral_loci <- c(1:nloci)[-adaptive_loci]
  
  #PCA to determine number of latent factors
  #if K is not specified it is calculated based on PCs
  if(is.null(K)){
    pc <- prcomp(gen)
    par(pty="s",mfrow=c(1,1))
    #if number of samples is greater than 100, only look at fits 100 PCs (shouldn't make a dif either way)
    if(nrow(gen)>100){
      eig <- pc$sdev[1:100]^2
    } else {
      #eig <- pc$sdev^2
      eig <- pc$sdev[1:20]^2
    }
    #estimate number of latent factors using quick.elbow (see general functions for description of how this function works)
    #this is a crude way to determine the number of latent factors that is based on an arbitrary "low" value 
    #(low defaults to 0.08, but this was too high imo so I changed it t0 0.05)
    K <- quick.elbow(eig, low = 0.1, max.pc = 0.8)
    var_explained = pc$sdev^2 / sum(pc$sdev^2)
    plot(var_explained, xlab = 'PC', ylab = "Variance explained")
    abline(v = K, col= "red", lty="dashed")
  }
}

nsamp = 81

```

```{r}
par(mfrow=c(length(npts), length(sampstrats)))
for(nsamp in npts){
  par(mfrow=c(1, length(sampstrats)))
  for(sampstrat in sampstrats){
    #subsample from data based on sampling strategy and number of samples
    subIDs <- get_samples(params[i,], params, sampstrat, nsamp)
    subgen <- gen[subIDs,]
    subgsd_df <- gsd_df[subIDs,]
    
    #run analysis using subsample
    sub_result <- run_lfmm(subgen, subgsd_df, loci_df, K = NULL)
    
  }
}
nsamp = 36




```
```{r}
#gen2 <- gen
gen <- subgen
#create temporary file with genotypes
write.geno(gen, here("data","temp_genotypes.geno"))
  
#Estimate admixture coefficients using sparse Non-Negative Matrix Factorization algorithms,
#Code for testing multiple K values:
maxK <- 30
obj.snmf <- snmf(here("data","temp_genotypes.geno"), K = 1:maxK, ploidy = 2, entropy = T, alpha = 100, project = "new")
  
#determining best K and picking best replicate for best K (source:https://chazhyseni.github.io/NALgen/post/determining_bestk/)
ce <- list()
for(k in 1:maxK) ce[[k]] <- cross.entropy(obj.snmf, K=k)
ce.K <- c()
for(k in 1:maxK) ce.K[k] <- min(ce[[k]])
diff <- ce.K[-1] - ce.K[-maxK]
slope <- exp(-diff) - 1
#K is selected based on the smallest slope value in the upper quartile
K <- min(which(slope <= quantile(slope)[4]))


plot(obj.snmf, col = "blue4", cex = 1.4, pch = 19)
abline(v = K, col = "red", lty = "dashed")

```
```{r}

run_lfmm <- function(gen, gsd_df, loci_df){
  
  #for readibility, just negates the in function
  `%notin%` <- Negate(`%in%`)
  
  #get adaptive loci
  loci_trait1 <- loci_df$trait1 + 1 #add one to convert from python to R indexing
  loci_trait2 <- loci_df$trait2 + 1 #add one to convert from python to R indexing
  adaptive_loci <- c(loci_trait1, loci_trait2)
  neutral_loci <- c(1:nloci)[-adaptive_loci]
  
  #gen matrix
  genmat = as.matrix(gen)
  #env matrix
  env1mat = as.matrix(gsd_df$env1)
  env2mat = as.matrix(gsd_df$env2)
  envmat = cbind(env1mat, env2mat)
  
  df <- data.frame()
  #TEST DIFFERENT K
  for(K in c(seq(2,nrow(gen)-5,10))){
    #BOTH ENV
    #run model
    lfmm_mod <- lfmm_ridge(genmat, envmat, K = K)
    #performs association testing using the fitted model:
    pv <- lfmm_test(Y = genmat, 
                    X = envmat, 
                    lfmm = lfmm_mod, 
                    calibrate = "gif")
    #adjust pvalues
    pvalues <- data.frame(env1=p.adjust(pv$calibrated.pvalue[,1], method="fdr"),
                          env2=p.adjust(pv$calibrated.pvalue[,2], method="fdr"))
    #env1 candidate loci
    #Identify LFMM cand loci (P)
    lfmm_loci1 <- which(pvalues[,"env1"] < 0.05) 
    #Identify negatives
    lfmm_neg1 <- which(pvalues[,"env1"] >= 0.05 | is.na(pvalues[,"env1"]))
    #check length makes sense
    stopifnot(length(lfmm_loci1) + length(lfmm_neg1) == ncol(gen))
    
    #get confusion matrix values
    #True Positives
    TP1 <- sum(lfmm_loci1 %in% loci_trait1)
    #False Positives
    FP1 <- sum(lfmm_loci1 %notin% loci_trait1)
    #True Negatives
    TN1 <- sum(lfmm_neg1 %notin% loci_trait1)
    #False Negatives
    FN1 <- sum(lfmm_neg1 %in% loci_trait1)
    #check sum makes sense
    stopifnot(sum(TP1, FP1, TN1, FN1) == ncol(gen))
    
    #env2 candidate loci
    #Identify LFMM cand loci
    lfmm_loci2 <- which(pvalues[,"env2"] < 0.05) 
    #Identify negatives
    lfmm_neg2 <- which(pvalues[,"env2"] >= 0.05 | is.na(pvalues[,"env2"]))
    #check length makes sense
    stopifnot(length(lfmm_loci2) + length(lfmm_neg2) == ncol(gen))
    
    #True Positives
    TP2 <- sum(lfmm_loci2 %in% loci_trait2)
    #False Positives
    FP2 <- sum(lfmm_loci2 %notin% loci_trait2)
    #True Negatives
    TN2 <- sum(lfmm_neg2 %notin% loci_trait2)
    #False Negatives
    FN2 <- sum(lfmm_neg2 %in% loci_trait2)
    #check length makes sense
    stopifnot(sum(TP2, FP2, TN2, FN2) == ncol(gen))
    
    #stats for all loci 
    lfmm_loci <- c(lfmm_loci1, lfmm_loci2)
    #calc confusion matrix
    TP <- TP1 + TP2
    FP <- FP1 + FP2
    TN <- TN1 + TN2
    FN <- FN1 + FN2
    #check sum makes sense
    stopifnot(sum(TP, FP, TN, FN) == 2*ncol(gen))
    
    #calc True Positive Rate (i.e. Sensitivity)
    TPRCOMBO <- TP/(TP + FN)
    #calc True Negative Rate (i.e. Specificity)
    TNRCOMBO <- TN/(TN + FP)
    #calc False Discovery Rate 
    FDRCOMBO <- FP/(FP + TP)
    #calc False Positive Rate 
    FPRCOMBO <- FP/(FP + TN)
    
    df <- rbind.data.frame(df, data.frame(K = K,
                      TPRCOMBO = TPRCOMBO, 
                      TNRCOMBO = TNRCOMBO,
                      FDRCOMBO = FDRCOMBO, 
                      FPRCOMBO = FPRCOMBO,
                      TOTALN = length(lfmm_loci), 
                      TOTALTP = TP, 
                      TOTALFP = FP, 
                      TOTALTN = TN,
                      TOTALFN = FN))
    }
 
  return(df)
}


```

```{r}
par(mfrow=c(length(npts), length(sampstrats)))
res_df <- data.frame()
for(nsamp in npts){
  for(sampstrat in sampstrats){
    #subsample from data based on sampling strategy and number of samples
    subIDs <- get_samples(params[i,], params, sampstrat, nsamp)
    subgen <- gen[subIDs,]
    subgsd_df <- gsd_df[subIDs,]
    
    #run analysis using subsample
    sub_result <- run_lfmm(subgen, subgsd_df, loci_df)
    plot(sub_result$K, sub_result$TPRCOMBO, type = "l")
    plot(sub_result$K, sub_result$FDRCOMBO, type = "l", col = "red")
    
    sub_result$nsamp <- nsamp
    sub_result$sampstrat <- sampstrat
    res_df <- rbind.data.frame(res_df, sub_result)
    
  }
}
 
```
```{r}
res_df$nsamp <- as.character(res_df$nsamp)
ggplot(res_df, aes(x=K, y=TPRCOMBO, colour=nsamp, shape = sampstrat,
  group=interaction(nsamp, sampstrat))) + 
  geom_point() + geom_line()
```
```{r fig.width=10, fig.height=10}
par(mfrow = c(length(npts), length(sampstrats)), pty = "s", mar = rep(4,4), oma=rep(0,4))
for(nsamp in npts){
  for(sampstrat in sampstrats){
    d <- res_df[res_df$nsamp == nsamp & res_df$sampstrat == sampstrat,]
    
    plot(d$K, d$TPRCOMBO, type="l", lwd=2,
         xlab = "K", 
         ylab = "TPR (blue) | FDR (red)", 
         main = paste0(nsamp, " | ", sampstrat), 
         ylim = c(0,1.1), 
         col = colorspace::adjust_transparency("#8000ff", alpha=0.5))
    lines(d$K, d$FDRCOMBO, type="l", lwd=2, col = colorspace::adjust_transparency("#ff0077", alpha=0.5))
    points(d$K, d$FDRCOMBO, col = "#ff0077", pch=19)
    points(d$K, d$TPRCOMBO, col = "#8000ff", pch=19)
    
  }
}
```
```{r fig.width=15, fig.height=15}
par(mfrow = c(length(npts), length(sampstrats)), pty = "s", mar = rep(4,4), oma=rep(0,4))
#i=40
i=36
for(nsamp in npts){
  for(sampstrat in sampstrats){
    #subsample from data based on sampling strategy and number of samples
    subIDs <- get_samples(params[i,], params, sampstrat, nsamp)
    subgen <- gen[subIDs,]
    subgsd_df <- gsd_df[subIDs,]
    
    
    pc <- prcomp(subgen)
    #if number of samples is greater than 100, only look at fits 100 PCs (shouldn't make a dif either way)
    if(nrow(subgen)>100){
      eig <- pc$sdev[1:100]^2
    } else {
      eig <- pc$sdev^2
    }
    
    #estimate number of latent factors using quick.elbow (see general functions for description of how this function works)
    #this is a crude way to determine the number of latent factors that is based on an arbitrary "low" value 
    #(low defaults to 0.08, but this was too high imo so I changed it t0 0.05)
    K <- quick.elbow(eig, low = 0.1, max.pc = 0.8)
    var_explained = pc$sdev^2 / sum(pc$sdev^2)
    plot(var_explained, xlab = '# PCs', ylab = "Variance explained", pch = 19, cex = 1, main = paste0(nsamp, " | ", sampstrat))
    lines(var_explained)
    #abline(v = K, col= "red", lty="dashed")
    
  }
}

```
```{r fig.width=15, fig.height=15}
par(mfrow = c(length(npts), length(sampstrats)), pty = "s", mar = rep(4,4), oma=rep(0,4))

i=36
for(nsamp in npts){
  for(sampstrat in sampstrats){
    #subsample from data based on sampling strategy and number of samples
    subIDs <- get_samples(params[i,], params, sampstrat, nsamp)
    subgen <- gen[subIDs,]
    subgsd_df <- gsd_df[subIDs,]
    
    
    pc <- prcomp(subgen)
    eig <- pc$sdev[1:20]^2
    
    #estimate number of latent factors using quick.elbow (see general functions for description of how this function works)
    #this is a crude way to determine the number of latent factors that is based on an arbitrary "low" value 
    #(low defaults to 0.08, but this was too high imo so I changed it t0 0.05)
    
    var_explained = eig / sum(pc$sdev^2)
    plot(var_explained, xlab = '# PCs', ylab = "Variance explained", pch = 19, cex = 1, main = paste0(nsamp, " | ", sampstrat))
    lines(var_explained)
    #K <- quick.elbow(eig, low = 0.08, max.pc = 0.8)
    #abline(v = K, col= "green", lty="dashed")
    #K <- quick.elbow(eig, low = 0.1, max.pc = 0.8)
    #abline(v = K, col= "blue", lty="dashed")
    
  }
}

```
